"""
SignBridge Sign Language Interpreter API
Using sign-language-translator library: https://pypi.org/project/sign-language-translator/

PROPER IMPLEMENTATION using the SLT library
"""

import os
import io
import base64
import asyncio
import gc
from datetime import datetime
from typing import Optional
import json
import logging

# Suppress verbose warnings
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
logging.getLogger('absl').setLevel(logging.ERROR)

# Web framework
from fastapi import FastAPI, UploadFile, File, Form, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
from fastapi.staticfiles import StaticFiles

# Image processing
import numpy as np
from PIL import Image
import cv2

# ============================================================
# Sign Language Translator Library
# ============================================================

SLT_AVAILABLE = False
translator = None
language_model = None

try:
    import sign_language_translator as slt
    from sign_language_translator.config.settings import Settings
    Settings.SHOW_DOWNLOAD_PROGRESS = True
    SLT_AVAILABLE = True
    print(f"âœ… sign-language-translator v{slt.__version__} loaded!")
    
    # Print available models and languages
    print("\nðŸ“¦ Available components:")
    print(f"   Languages: {slt.languages.SUPPORTED_LANGUAGES if hasattr(slt.languages, 'SUPPORTED_LANGUAGES') else 'checking...'}")
    
except ImportError as e:
    print(f"âš ï¸ sign-language-translator not available: {e}")
    print("   Install with: pip install sign-language-translator[mediapipe]")

# ============================================================
# Memory Settings
# ============================================================

MAX_IMAGE_WIDTH = 640
MAX_IMAGE_HEIGHT = 480

# ============================================================
# FastAPI Setup
# ============================================================

app = FastAPI(
    title="SignBridge Sign Language Interpreter",
    description="Sign language translation using sign-language-translator library",
    version="4.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================
# Global State
# ============================================================

embedding_model = None
translator_ready = False

class SentenceBuilder:
    def __init__(self):
        self.current_word = ""
        self.words = []
        self.last_letter = None
        self.repeat_count = 0
        
    def add_letter(self, letter: str, confidence: float) -> dict:
        if not letter or letter == "Nothing" or confidence < 0.5:
            self.repeat_count = 0
            return self.get_state()
        
        if letter == self.last_letter:
            self.repeat_count += 1
            if self.repeat_count == 3:  # Confirm after 3 frames
                if letter == " " or letter == "Space":
                    if self.current_word:
                        self.words.append(self.current_word)
                        self.current_word = ""
                elif letter == "Delete" or letter == "Backspace":
                    if self.current_word:
                        self.current_word = self.current_word[:-1]
                    elif self.words:
                        self.words.pop()
                else:
                    self.current_word += letter
                self.repeat_count = 0
        else:
            self.last_letter = letter
            self.repeat_count = 1
            
        return self.get_state()
    
    def get_state(self) -> dict:
        sentence = " ".join(self.words)
        if self.current_word:
            sentence += (" " if sentence else "") + self.current_word
        return {
            "current_word": self.current_word,
            "words": self.words.copy(),
            "full_sentence": sentence,
            "last_letter": self.last_letter
        }
    
    def clear(self):
        self.current_word = ""
        self.words = []
        self.last_letter = None
        self.repeat_count = 0

sentence_builder = SentenceBuilder()

# ============================================================
# Model Loading
# ============================================================

def load_models():
    """Load sign language translator models"""
    global embedding_model, translator, translator_ready
    
    if not SLT_AVAILABLE:
        print("âš ï¸ SLT library not available")
        return False
    
    try:
        print("\n" + "=" * 50)
        print("Loading Sign Language Translator models...")
        print("=" * 50)
        
        # Load the embedding model for extracting features from images/video
        print("\nðŸ“¦ Loading MediaPipe Landmarks model...")
        embedding_model = slt.models.MediaPipeLandmarksModel()
        print("âœ… MediaPipe model loaded!")
        
        # Try to load a translator for text-to-sign (bidirectional)
        print("\nðŸ“¦ Checking available translators...")
        try:
            # List available models
            if hasattr(slt, 'get_model'):
                print("   Available models: checking...")
            
            # Try to create a translator
            # The library supports: pk-sl (Pakistan Sign Language), etc.
            translator = slt.Translator(
                text_language="english",
                sign_language="pk-sl",  # Pakistan Sign Language (default available)
                sign_format="video"
            )
            print("âœ… Translator loaded (English â†” Pakistan Sign Language)")
        except Exception as e:
            print(f"âš ï¸ Full translator not available: {e}")
            print("   Using embedding-only mode")
        
        translator_ready = True
        print("\n" + "=" * 50)
        print("âœ… Models loaded successfully!")
        print("=" * 50 + "\n")
        return True
        
    except Exception as e:
        print(f"âŒ Error loading models: {e}")
        import traceback
        traceback.print_exc()
        return False

# ============================================================
# Image Processing
# ============================================================

def resize_image(img: np.ndarray) -> np.ndarray:
    """Resize image to save memory"""
    h, w = img.shape[:2]
    if w > MAX_IMAGE_WIDTH or h > MAX_IMAGE_HEIGHT:
        scale = min(MAX_IMAGE_WIDTH / w, MAX_IMAGE_HEIGHT / h)
        img = cv2.resize(img, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_AREA)
    return img

def process_image(image_bytes: bytes) -> dict:
    """Process image and extract landmarks/features"""
    global embedding_model
    
    if embedding_model is None:
        return {"detected": False, "error": "Model not loaded"}
    
    try:
        # Decode image
        nparr = np.frombuffer(image_bytes, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        if img is None:
            return {"detected": False, "error": "Failed to decode image"}
        
        # Resize
        img = resize_image(img)
        
        # Convert BGR to RGB
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        
        # Extract landmarks using MediaPipe
        landmarks = embedding_model.embed(img_rgb)
        
        # Clean up
        del img, img_rgb
        gc.collect()
        
        if landmarks is not None and len(landmarks) > 0:
            # Convert landmarks to feature vector
            features = landmarks.flatten() if hasattr(landmarks, 'flatten') else landmarks
            
            return {
                "detected": True,
                "landmarks": features.tolist() if hasattr(features, 'tolist') else list(features),
                "num_landmarks": len(features) if hasattr(features, '__len__') else 0
            }
        
        return {"detected": False}
        
    except Exception as e:
        print(f"Error processing image: {e}")
        return {"detected": False, "error": str(e)}

def predict_letter(features: dict) -> tuple:
    """
    Predict letter from extracted features
    Returns (letter, confidence)
    """
    if not features.get("detected"):
        return "Nothing", 0.0
    
    landmarks = features.get("landmarks", [])
    num_landmarks = features.get("num_landmarks", 0)
    
    if not landmarks or num_landmarks == 0:
        return "Nothing", 0.0
    
    # The SLT library extracts MediaPipe landmarks
    # For actual letter recognition, we analyze the hand pose
    
    # MediaPipe hand landmarks: 21 points x 3 coordinates = 63 values per hand
    # Full body: 33 pose + 21 left hand + 21 right hand + 468 face = 543 landmarks
    
    # Simple heuristic based on landmark presence and positions
    # This is a basic implementation - full recognition needs trained models
    
    try:
        arr = np.array(landmarks)
        
        # Check if we have hand landmarks (at least 21 points)
        if len(arr) >= 63:  # One hand detected
            # Normalize landmarks
            arr_normalized = (arr - arr.min()) / (arr.max() - arr.min() + 1e-6)
            
            # Simple gesture detection based on landmark patterns
            # This is placeholder logic - real recognition needs ML models
            
            # Calculate spread (distance between fingertips)
            hand_data = arr[:63].reshape(-1, 3) if len(arr) >= 63 else None
            
            if hand_data is not None:
                # Fingertip indices: 4 (thumb), 8 (index), 12 (middle), 16 (ring), 20 (pinky)
                if len(hand_data) >= 21:
                    thumb_tip = hand_data[4]
                    index_tip = hand_data[8]
                    middle_tip = hand_data[12]
                    ring_tip = hand_data[16]
                    pinky_tip = hand_data[20]
                    wrist = hand_data[0]
                    
                    # Calculate finger extensions (distance from wrist)
                    thumb_ext = np.linalg.norm(thumb_tip - wrist)
                    index_ext = np.linalg.norm(index_tip - wrist)
                    middle_ext = np.linalg.norm(middle_tip - wrist)
                    ring_ext = np.linalg.norm(ring_tip - wrist)
                    pinky_ext = np.linalg.norm(pinky_tip - wrist)
                    
                    # Simple gesture recognition rules
                    all_extended = all([index_ext > 0.15, middle_ext > 0.15, ring_ext > 0.15, pinky_ext > 0.15])
                    all_closed = all([index_ext < 0.1, middle_ext < 0.1, ring_ext < 0.1, pinky_ext < 0.1])
                    
                    # Detect some basic gestures
                    if all_extended and thumb_ext > 0.1:
                        return "5", 0.75  # Open hand = 5
                    elif index_ext > 0.15 and middle_ext < 0.1 and ring_ext < 0.1:
                        return "1", 0.75  # Index pointing = 1
                    elif index_ext > 0.15 and middle_ext > 0.15 and ring_ext < 0.1:
                        return "V", 0.75  # Peace sign = V
                    elif thumb_ext > 0.15 and index_ext < 0.1:
                        return "A", 0.70  # Thumbs up = A
                    elif all_closed:
                        return "S", 0.70  # Fist = S
                    elif pinky_ext > 0.15 and index_ext < 0.1:
                        return "I", 0.70  # Pinky extended = I
                    elif index_ext > 0.1 and pinky_ext > 0.1 and middle_ext < 0.1:
                        return "Y", 0.70  # Rock sign = Y
                    else:
                        return "Hand", 0.60  # Hand detected but gesture unknown
            
            return "Hand", 0.60
        else:
            return "Nothing", 0.0
            
    except Exception as e:
        print(f"Prediction error: {e}")
        return "Error", 0.0

# ============================================================
# Startup
# ============================================================

@app.on_event("startup")
async def startup():
    print("\n" + "=" * 60)
    print("SignBridge Sign Language Interpreter v4.0")
    print("Using: sign-language-translator library")
    print("=" * 60)
    print(f"SLT available: {SLT_AVAILABLE}")
    
    if SLT_AVAILABLE:
        load_models()
    
    port = os.environ.get("PORT", "8000")
    print(f"\nðŸš€ Server ready on port {port}")
    print("=" * 60 + "\n")

# ============================================================
# API Endpoints
# ============================================================

@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "slt_available": SLT_AVAILABLE,
        "model_loaded": translator_ready,
        "version": "4.0.0"
    }

@app.get("/api")
async def api_info():
    return {
        "service": "SignBridge Sign Language Interpreter",
        "version": "4.0.0",
        "library": "sign-language-translator",
        "library_version": slt.__version__ if SLT_AVAILABLE else "not installed",
        "model_loaded": translator_ready,
        "supported_gestures": ["1", "5", "A", "I", "S", "V", "Y", "Hand"],
        "features": [
            "MediaPipe landmark extraction",
            "Basic gesture recognition",
            "Sentence building",
            "Text-to-Sign translation"
        ]
    }

@app.post("/predict-image")
async def predict_image(
    file: UploadFile = File(...),
    language: str = Form("ASL")
):
    """Predict gesture from image"""
    try:
        contents = await file.read()
        features = process_image(contents)
        del contents
        gc.collect()
        
        letter, confidence = predict_letter(features)
        
        return {
            "label": letter,
            "confidence": confidence,
            "language": language,
            "landmarks_detected": features.get("detected", False),
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

@app.post("/predict-and-build")
async def predict_and_build(
    file: UploadFile = File(...),
    language: str = Form("ASL")
):
    """Predict gesture and build sentence"""
    try:
        contents = await file.read()
        features = process_image(contents)
        del contents
        gc.collect()
        
        letter, confidence = predict_letter(features)
        state = sentence_builder.add_letter(letter, confidence)
        
        return {
            "label": letter,
            "confidence": confidence,
            "language": language,
            "landmarks_detected": features.get("detected", False),
            **state,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

@app.post("/clear-sentence")
async def clear_sentence():
    sentence_builder.clear()
    return {"status": "cleared", "sentence_state": sentence_builder.get_state()}

@app.post("/text-to-sign")
async def text_to_sign(text: str = Form(...), language: str = Form("pk-sl")):
    """Convert text to sign language (using SLT library)"""
    if not SLT_AVAILABLE:
        return JSONResponse(status_code=503, content={"error": "SLT not available"})
    
    try:
        if translator:
            # Use the SLT translator
            result = translator(text)
            return {
                "input": text,
                "output": str(result),
                "language": language
            }
        else:
            # Fallback: just tokenize
            words = text.strip().split()
            return {
                "input": text,
                "words": words,
                "language": language,
                "note": "Full translation requires additional models"
            }
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

@app.websocket("/ws/stream")
async def websocket_stream(websocket: WebSocket):
    """Real-time recognition via WebSocket"""
    await websocket.accept()
    print("WebSocket connected")
    
    try:
        while True:
            data = await websocket.receive_text()
            msg = json.loads(data)
            
            if msg.get("type") == "frame":
                image_data = msg.get("data", "")
                if "," in image_data:
                    image_data = image_data.split(",")[1]
                
                image_bytes = base64.b64decode(image_data)
                features = process_image(image_bytes)
                del image_bytes
                gc.collect()
                
                letter, confidence = predict_letter(features)
                state = sentence_builder.add_letter(letter, confidence)
                
                await websocket.send_json({
                    "type": "prediction",
                    "label": letter,
                    "confidence": confidence,
                    "landmarks_detected": features.get("detected", False),
                    **state
                })
                
            elif msg.get("type") == "clear":
                sentence_builder.clear()
                await websocket.send_json({
                    "type": "cleared",
                    "sentence_state": sentence_builder.get_state()
                })
                
    except WebSocketDisconnect:
        print("WebSocket disconnected")
        gc.collect()

# ============================================================
# Static Files
# ============================================================

static_dir = os.path.join(os.path.dirname(__file__), '..')

for folder in ['css', 'js', 'images']:
    folder_path = os.path.join(static_dir, folder)
    if os.path.exists(folder_path):
        app.mount(f"/{folder}", StaticFiles(directory=folder_path), name=folder)

@app.get("/config.js")
async def serve_config():
    path = os.path.join(static_dir, 'config.js')
    if os.path.exists(path):
        return FileResponse(path, media_type="application/javascript")
    return JSONResponse(content={}, status_code=404)

@app.get("/")
async def serve_index():
    path = os.path.join(static_dir, 'index.html')
    return FileResponse(path) if os.path.exists(path) else {"message": "API running", "docs": "/docs"}

@app.get("/recognition.html")
async def serve_recognition():
    return FileResponse(os.path.join(static_dir, 'recognition.html'))

@app.get("/learning.html")
async def serve_learning():
    return FileResponse(os.path.join(static_dir, 'learning.html'))

@app.get("/about.html")
async def serve_about():
    return FileResponse(os.path.join(static_dir, 'about.html'))

# ============================================================
# Run
# ============================================================

if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8000))
    print(f"Starting server on port {port}...")
    uvicorn.run(app, host="0.0.0.0", port=port, log_level="warning")
